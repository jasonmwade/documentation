LizardFS provides an open-source way to join multiple random disks into one logical, global, distributed 
storage volume. The volume support data chunking and replication settings that are dynamically set while 
running.

Chunkserver is the name of the instance that holds an actual disk. The chunkserver can be a VM, physical 
system, and ARM or X86. The disk configuration is set by a simple config file on each chunkserver pointing 
to an existing mounted file system. The file system can be any standard Linux-supported low-level file 
system such as XFS or BTRFS. The mount points are simply mounted to the host and the configuration file 
pointed to the location to store the chunk data. Since this file system replicates data from other systems,
the disk input/output should be maximized where possible by using SSD or NVME. Spinning hard disks will 
work also, but the overall larger volume will be much slower.

Master server is the central metadata location. This can be replicated with a metadatalogger service 
that can be run on a separate system. Master server also runs the web interface for LizardFS that shows
all kinds of interesting data. 

Home Layout
Chunkserver #1 - Raspberry Pi 4 with 4GB of ram running Ubuntu 22.04 LTS. 
Since Ubuntu 22.04 does not have LizardFS in the repos for ARM64, a custom docker container is created using 
ubuntu:20.04 and the configurations are added to this container. docker-compose is used to spin up the
container and manage the process. 
The volumes are as follows:
/mnt/data/lfs:/data   # 3TB volume connected to a USB3 docking station
/mnt/mfs-data:/mnt/mfs-data  # 512GB boot volume connected to a USB3 docking station

Chunkserver #2 - HP ProDesk 600 G1 SFF with 8GB of ram. 
The volumes are as follows:
/mnt/2/mfs03 # 8TB spinning disk
/mnt/data01/lfs # 3TB spinning disk
/mnt/data02/lfs # 1TB spinning disk

Chunkserver #3 - ASRock DeskMini 110M with 16GB of ram.
This host is also running Proxmox and contains a VM running the master server including the web interface. 
The volumes are as follows: 
/bigdata/mfs # 8TB spinning disk

Overall configuration
The above configuration gives 21TB total space and around 15TB of usable space. Taking down one host does
not result in any storage outage. An extended outage or failure causes the chunks to start replicating
as fast as the network and disks can handle to maintain the goal of 2. Higher goals are possible also, but 
depends on the number of separate systems available.

General guidance
* Have at least a goal of 2 (lizardfs set goal -r 2 /lizardfs/directory) to ensure data resiliency
* Reads and writes are notably slower than you would expect. The max throughput with a goal of 2 (2 copies of 
each chunk) with the above setup is around 30MB/sec.
* Use at least 1Gb networking on all interfaces and between switches. Older Raspberry Pi devices have 100Mb 
networking and are NOT suitable for this application
* mfsmount is used to mount the volume. To get the volume on a Windows system, create a linux VM that mounts
the storage and then runs Samba to share it to Windows. The permissions are difficult if AD or LDAP are used,
but simple Samba authentication is the same as any other file system.
* The more disks the faster the file system. 
* Minimum of 3 servers will ensure data resiliency with a goal of 2. One failure in a 3 server configuration 
leaves enough systems to ensure the goal can be met.
